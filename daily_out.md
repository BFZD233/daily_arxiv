|paper|code|abstract|
|---|---|---|
|[trellis bma: coded trace reconstruction on ids channels for dna storage](https://arxiv.org/abs/2107.06440)|[clustered-nanopore-reads-dataset](https://github.com/microsoft/clustered-nanopore-reads-dataset)|sequencing a dna strand, as part of the read process in dna storage, produces multiple noisy copies which can be combined to produce better estimates of the original strand; this is called trace reconstruction. one can reduce the error rate further by introducing redundancy in the write sequence and this is called coded trace reconstruction. in this paper, we model the dna storage channel as an insertion-deletion-substitution (ids) channel and design both encoding schemes and low-complexity decoding algorithms for coded trace reconstruction.   we introduce trellis bma, a new reconstruction algorithm whose complexity is linear in the number of traces, and compare its performance to previous algorithms. our results show that it reduces the error rate on both simulated and experimental data. the performance comparisons in this paper are based on a new dataset of traces that will be publicly released with the paper. our hope is that this dataset will enable research progress by allowing objective comparisons between candidate algorithms.|
|[on confidence sequences for bounded random processes via universal gambling strategies](https://arxiv.org/abs/2207.12382)|[confidence-sequence-via-gambling](https://github.com/jongharyu/confidence-sequence-via-gambling)|this paper considers the problem of constructing a confidence sequence, which is a sequence of confidence intervals that hold uniformly over time, for estimating the mean of bounded real-valued random processes. this paper revisits the gambling-based approach established in the recent literature from a natural \emph{two-horse race} perspective, and demonstrates new properties of the resulting algorithm induced by cover (1991)'s universal portfolio. the main result of this paper is a new algorithm based on a mixture of lower bounds, which closely approximates the performance of cover's universal portfolio with constant per-round time complexity. a higher-order generalization of a lower bound on a logarithmic function in (fan et al., 2015), which is developed as a key technique for the proposed algorithm, may be of independent interest.|
|[fast erasure decoder for hypergraph product codes](https://arxiv.org/abs/2208.01002)|[pruned-peeling-and-vh-decoder](https://github.com/nicholas-connolly/pruned-peeling-and-vh-decoder)|we propose a decoder for the correction of erasures with hypergraph product codes, which form one of the most popular families of quantum ldpc codes. our numerical simulations show that this decoder provides a close approximation of the maximum likelihood decoder that can be implemented in o(n^2) bit operations where n is the length of the quantum code. a probabilistic version of this decoder can be implemented in o(n^1.5) bit operations.|
|[improved field size bounds for higher order mds codes](https://arxiv.org/abs/2212.11262)|[mds3-groebner](https://github.com/jbrakensiek/mds3-groebner)|higher order mds codes are an interesting generalization of mds codes recently introduced by brakensiek, gopi and makam (ieee trans. inf. theory 2022). in later works, they were shown to be intimately connected to optimally list-decodable codes and maximally recoverable tensor codes. therefore (explicit) constructions of higher order mds codes over small fields is an important open problem. higher order mds codes are denoted by $\operatorname{mds}(\ell)$ where $\ell$ denotes the order of generality, $\operatorname{mds}(2)$ codes are equivalent to the usual mds codes. the best prior lower bound on the field size of an $(n,k)$-$\operatorname{mds}(\ell)$ codes is $\omega_\ell(n^{\ell-1})$, whereas the best known (non-explicit) upper bound is $o_\ell(n^{k(\ell-1)})$ which is exponential in the dimension.   in this work, we nearly close this exponential gap between upper and lower bounds. we show that an $(n,k)$-$\operatorname{mds}(3)$ codes requires a field of size $\omega_k(n^{k-1})$, which is close to the known upper bound. using the connection between higher order mds codes and optimally list-decodable codes, we show that even for a list size of 2, a code which meets the optimal list-decoding singleton bound requires exponential field size; this resolves an open question from shangguan and tamo (stoc 2020 / siam j. on computing 2023).   we also give explicit constructions of $(n,k)$-$\operatorname{mds}(\ell)$ code over fields of size $n^{(\ell k)^{o(\ell k)}}$. the smallest non-trivial case where we still do not have optimal constructions is $(n,3)$-$\operatorname{mds}(3)$. in this case, the known lower bound on the field size is $\omega(n^2)$ and the best known upper bounds are $o(n^5)$ for a non-explicit construction and $o(n^{32})$ for an explicit construction. in this paper, we give an explicit construction over fields of size $o(n^3)$ which comes very close to being optimal.|
|[energy-efficient beamforming for riss-aided communications: gradient based meta learning](https://arxiv.org/abs/2311.06861)|[GMML](https://github.com/fenghaozhu/GMML)|reconfigurable intelligent surfaces (riss) have become a promising technology to meet the requirements of energy efficiency and scalability in future six-generation (6g) communications. however, a significant challenge in riss-aided communications is the joint optimization of active and passive beamforming at base stations (bss) and riss respectively. specifically, the main difficulty is attributed to the highly non-convex optimization space of beamforming matrices at both bss and riss, as well as the diversity and mobility of communication scenarios. to address this, we present a greenly gradient based meta learning beamforming (gmlb) approach. unlike traditional deep learning based methods which take channel information directly as input, gmlb feeds the gradient of sum rate into neural networks. coherently, we design a differential regulator to address the phase shift optimization of riss. moreover, we use the meta learning to iteratively optimize the beamforming matrices of bss and riss. these techniques make the proposed method to work well without requiring energy-consuming pre-training. simulations show that gmlb could achieve higher sum rate than that of typical alternating optimization algorithms with the energy consumption by two orders of magnitude less.|
|[understanding is compression](https://arxiv.org/abs/2407.07723)|[medal](https://github.com/mcGill-NLP/medal)|modern data compression methods are slowly reaching their limits after 80 years of research, millions of papers, and wide range of applications. yet, the extravagant 6g communication speed requirement raises a major open question for revolutionary new ideas of data compression.   we have previously shown all understanding or learning are compression, under reasonable assumptions. large language models (llms) understand data better than ever before. can they help us to compress data?   the llms may be seen to approximate the uncomputable solomonoff induction. therefore, under this new uncomputable paradigm, we present lmcompress. lmcompress shatters all previous lossless compression algorithms, doubling the lossless compression ratios of jpeg-xl for images, flac for audios, and h.264 for videos, and quadrupling the compression ratio of bz2 for texts. the better a large model understands the data, the better lmcompress compresses.|